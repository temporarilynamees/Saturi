from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

# ğŸ’¡ [ê°€ì¥ ì¤‘ìš”!] ê°ì ìì‹ ì˜ ì‹¤í—˜ì„ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ ì´ë¦„ì„ ì •í•©ë‹ˆë‹¤.
# wandb ëŒ€ì‹œë³´ë“œì™€ êµ¬ê¸€ ë“œë¼ì´ë¸Œ í´ë”ëª…ì— ì´ ì´ë¦„ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.
# í˜•ì‹ ì˜ˆì‹œ: "ì‹¤í—˜ìì´ë¦„-í•µì‹¬íŒŒë¼ë¯¸í„°-ê°’"
my_run_name = "Chulsoo-lr_3e-5-epochs_12"


# ëª¨ë¸ ì´ˆê¸°í™”
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# TPU í•™ìŠµì„ ìœ„í•œ TrainingArguments ì„¤ì •
training_args = Seq2SeqTrainingArguments(
    # ğŸ’¡ ê²°ê³¼ê°€ ì €ì¥ë  ê²½ë¡œ. run_nameì— ë”°ë¼ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.
    output_dir=f"/content/drive/MyDrive/kobart_experiments/{my_run_name}",
    optim="adafactor",

    # ğŸ’¡ wandbì— ë³´ê³ í•˜ê³ , run_nameì„ ì„¤ì •í•©ë‹ˆë‹¤.
    report_to="wandb",
    run_name=my_run_name,

    # --- ê°ì ë‹¤ë¥´ê²Œ ì„¤ì •í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
    learning_rate=3e-5,
    num_train_epochs=12,
    per_device_train_batch_size=32,
    weight_decay=0.01,

    # --- ê³µí†µ ì„¤ì • ---
    tpu_num_cores=8,
    per_device_eval_batch_size=32,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_steps=200,
    save_total_limit=2,
    predict_with_generate=True,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

# Trainer ê°ì²´ ìƒì„±
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
)

# ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘
print(f"âœ… ì‹¤í—˜ '{my_run_name}' í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
trainer.train()

print(f"ğŸ‰ ì‹¤í—˜ '{my_run_name}' í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ Driveì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
